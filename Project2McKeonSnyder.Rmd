---
title: "Project2McKeonSnyder"
author: "Owen Snyder"
date: '2022-07-05'
output: github_document
---

Render Function
```{r eval=FALSE}
rmarkdown::render("Project2McKeonSnyder.Rmd",
                  output_format = "github_document",
                  output_file = "README.md",
                  output_options = list(
                    html_preview = FALSE, toc = TRUE, toc_depth = 2, toc_float = TRUE)
)

```


# Introduction
This is a full report of Project 2 conducted by Ashlee McKeon and Owen Snyder.

## Our Goal and Purpose

The main goal of this project is to create predictive models based on data from an [online news popularity site](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity). The data includes about two years worth of information regarding *Mashable* articles. This data is summarized by a set of 61 categorical and quantitative variables that will eventually help us predict the number of shares a *Mashable* article gets. We will only use a subset of these variables to help predict the shares variable using a variety of predictive models such as Random Forrest and Boosted Tree models. A main variable of interest will be the... 



## Variable Selection

Because of the large variety of variables this data set includes, we decided to only consider a subset of the variables to work with. However, it is important to note one specific set of variables we will be basing our analysis on. We will be starting with the *data_channel_is_** variables to see if they have any relation to the number of shares and the rest of the variables we select.  

The following our are variables of interest:  

* **shares** - Number of shares (this is our target variable!) 
* **n_tokens_title** - Number of words in the title
* **average_token_length** - Average length of the words in the content
* **global_rate_positive_words** - Rate of positive words in the content
* **global_rate_negative_words** - Rate of negative words in the content

For categorical variables, we will be using the following varibles. In fact, we will be combining the days of the week into one variable called **dayOfWeek** and the method of its creation will be described as it arises.

* **weekday_is_monday** - Was the article published on a Monday?
* **weekday_is_tuesday** - Was the article published on a Tuesday?
* **weekday_is_wednesday** - Was the article published on a Wednesday?
* **weekday_is_thursday** - Was the article published on a Thursday?
* **weekday_is_friday** - Was the article published on a Friday?
* **weekday_is_saturday** - Was the article published on a Saturday?
* **weekday_is_sunday** - Was the article published on a Sunday?

Finally, the six data channels we will be working with come from these key variables. Also, note that we  have created one variable to represent each data channel named **dataChannel**. Its method of creation will also be described below as it arises.  

* **data_channel_is_lifestyle** - Is data channel 'Lifestyle'?
* **data_channel_is_entertainment** - Is data channel 'Entertainment'?
* **data_channel_is_bus** - Is data channel 'Business'?
* **data_channel_is_socmed** - Is data channel 'Social Media'?
* **data_channel_is_tech** - Is data channel 'Tech'?
* **data_channel_is_world** - Is data channel 'World'?

## Methods

Before we get started, it is important to briefly describe the methods wee used for analysis.  

* We will first read in the data set and manipulate the necessary variables.
* The data will be separated into a training and test set. This will be a 70/30 split.
* We will include some contingency tables at each setting of our categorical variables to get a better idea of the data at hand. We will also include summary statistics for quantitative variables. The data for these summaries will be used via the *training* data.
* We will include a variety of graphs to better visualize our data. These graphs will be described in a more subjective manner so that automation of each setting of this report makes more sense. We will still be using the *training* data.
* Finally, we will be using machine learning methods via Random Forrest Boosted Tree models. These models will be explained in greater detail in their respective section.

## Packages

Below is a list of necessary packages that will aid our data analysis and modeling.  

* `tidyverse` is a collection of useful packages designed for data science.
* `ggplot2` is an amazing way to create visually pleasing and informative graphics.
* `caret` is a set of functions that streamline the process of implementing machine learning methods.
* `rmarkdown` is a useful package for creating R Markdown documents in a variety of formats.
* `knitr` is a useful package to integrate computing and reporting.

```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(caret)
library(rmarkdown)
library(knitr)

```



# Read in Data

```{r warning=FALSE,message=FALSE}
newsData <- read_csv("/Users/owensnyder/Desktop/OnlineNewsPopularity.csv")
newsData
```

# Create New Variables

Because we are working with six different channels across six different columns of data, we figured it would be easier to combine every channel into one column. That way, we will be working with one single channel variable that takes on unique values of Lifestyle, Entertainment, Business, Social Media, Tech, and World. This will make automation and analysis more clear. Note that the original data tells us that a data for either channel is 1 or 0. meaning that 1 means "yes" and 0 means "no".

```{r warning=FALSE,message=FALSE}
## use mutate() to create a new channel variable
newsData <- newsData %>% mutate(dataChannel = 
                              ifelse(data_channel_is_bus==1, "Business",
                              ifelse(data_channel_is_entertainment==1, "Entertainment",
                              ifelse(data_channel_is_lifestyle==1, "Lifestyle",
                              ifelse(data_channel_is_socmed==1, "Social Media",
                              ifelse(data_channel_is_tech==1, "Tech",
                              ifelse(data_channel_is_world==1, "World", "Other")))))))

## upon inspection, there are NA values present, this could be due to some articles not having
## one of these specific categories



## convert to a factor for analysis??

#newsData

#summary(newsData)
```

# Split Data: Train/Test Set

# Numerical Summaries



# Contingency Tables
